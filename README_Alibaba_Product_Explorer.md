# Alibaba New Arrivals Product Explorer - Final Report and Usage Guide

## 1. Introduction

This document provides a summary of the Alibaba New Arrivals Product Explorer tool, its key features, and instructions for setup, usage, and maintenance.

The tool is designed to automatically scrape and categorize daily "New Arrivals" from Alibaba, allowing users to filter results by niche or keyword. It features smart categorization using NLP, a 30-day rolling archive of products, and a favorites/tracking system.

## 2. Key Features Implemented

*   **Product Scraping:** A Python script (`scraper.py`) using Playwright to fetch new product listings from Alibaba. It extracts product name, URL, image, and price.
*   **NLP-based Categorization & Clustering:** A Python script (`nlp_processor.py`) that processes scraped product names using NLTK and scikit-learn to perform TF-IDF vectorization and K-Means clustering. This assigns a `cluster_id` to each product.
*   **Web Application (Flask):** A Flask application (`main.py`) provides a user interface to:
    *   View product listings with pagination.
    *   Search products by keywords.
    *   Filter products by their assigned NLP cluster ID.
    *   Add/remove products to/from a "Favorites" list (currently for a single default user).
    *   Manually trigger data loading (simulated scraper and NLP run) and archival.
*   **Database:** SQLite is used to store product information, categories (though not fully utilized for smart categories yet beyond cluster IDs), and user favorites. The schema is defined in `models.py`.
*   **30-Day Rolling Archive:** Products older than 30 days (based on their `last_scraped_date`) are marked as inactive (`is_active = False`) and are not displayed in the main product listings. This is handled within the `load_scraped_data_to_db` function in `main.py`.
*   **User Interface:** HTML templates (`base.html`, `index.html`, `favorites.html`) styled with Bootstrap provide a clean and functional interface.

## 3. Project Structure

The project is organized in the `alibaba_explorer` directory:

```
alibaba_explorer/
├── scraper.py                # Alibaba product scraper script
├── nlp_processor.py          # NLP script for categorization/clustering
├── categorized_products.json # Output of NLP processor, input for DB loading
├── scraped_products.json     # Output of scraper, input for NLP processor
├── database_schema.md        # Description of the database tables
├── project_requirements.md   # Initial project requirements document
├── todo.md                   # Development checklist
├── venv/                     # Python virtual environment (created by Flask template)
├── src/
│   ├── main.py               # Main Flask application file
│   ├── base.html             # Base HTML template
│   ├── index.html            # Product listing page template
│   ├── favorites.html        # Favorites page template
│   ├── static/
│   │   └── style.css         # Basic CSS (can be expanded)
│   └── models/
│       └── models.py         # SQLAlchemy database models
└── requirements.txt          # Python dependencies for the Flask app (generated by template)
```

## 4. Setup and Running the Application

**Prerequisites:**
*   Python 3.10+
*   pip (Python package installer)
*   A modern web browser

**Steps:**

1.  **Extract the Project:** Unzip the provided `alibaba_explorer.zip` file to a desired location on your computer.

2.  **Set up Python Virtual Environment (Recommended):**
    Open a terminal or command prompt, navigate to the `alibaba_explorer` directory.
    ```bash
    python3 -m venv venv
    source venv/bin/activate  # On Windows: venv\Scripts\activate
    ```

3.  **Install Dependencies:**
    *   **For the Flask Web Application:**
        Navigate to the `alibaba_explorer` directory (if not already there) where `requirements.txt` (from the Flask template) and other project files are located.
        ```bash
        pip install -r requirements.txt # Installs Flask, SQLAlchemy, etc.
        pip install Flask-SQLAlchemy PyMySQL # Ensure these are explicitly listed or covered
        ```
    *   **For the Scraper (`scraper.py`):**
        ```bash
        pip install playwright pandas
        playwright install --with-deps # Installs browser binaries
        ```
    *   **For the NLP Processor (`nlp_processor.py`):**
        ```bash
        pip install nltk scikit-learn pandas
        ```
        The NLP script will attempt to download necessary NLTK data (wordnet, stopwords, punkt, punkt_tab) on its first run.

4.  **Initialize the Database:**
    Navigate to the `alibaba_explorer/src` directory.
    Set the `FLASK_APP` environment variable and run the init-db command:
    ```bash
    export FLASK_APP=main.py  # On Windows: set FLASK_APP=main.py
    python3 -m flask init-db
    ```
    This will create an `alibaba_explorer.db` SQLite file in the `src` directory.

5.  **Run the Scraper and NLP Processor (Initial Data Population):**
    *   **Run the Scraper:**
        Navigate to the `alibaba_explorer` directory.
        ```bash
        python3 scraper.py
        ```
        This will create/update `scraped_products.json`.
    *   **Run the NLP Processor:**
        ```bash
        python3 nlp_processor.py
        ```
        This will read `scraped_products.json` and create/update `categorized_products.json` with cluster IDs.

6.  **Load Data into the Web Application Database:**
    Navigate to the `alibaba_explorer/src` directory.
    ```bash
    python3 -m flask load-data
    ```
    This loads data from `categorized_products.json` into the database and performs initial archival.

7.  **Run the Flask Web Application:**
    Navigate to the `alibaba_explorer/src` directory.
    ```bash
    python3 main.py
    ```
    The application will be accessible at `http://0.0.0.0:5001` or `http://localhost:5001` in your web browser.

## 5. Using the Application

*   **Homepage:** Displays paginated product listings. You can search by keyword and filter by cluster ID.
*   **Favorites Page:** Shows products you have marked as favorites.
*   **Load/Refresh Products Button:** This button in the navigation bar simulates running the scraper and NLP processor by reloading data from the `categorized_products.json` file into the database and then running the archival process. For actual daily updates, you need to run `scraper.py` and `nlp_processor.py` externally (see section 6).
*   **Process NLP Clusters Button:** This button is largely redundant if the "Load/Refresh Products" button is used, as that process now includes loading the NLP-processed file. It was kept as a separate trigger during development.

## 6. Automating Daily Updates (User-side Scheduling)

As the hosting environment does not support scheduled tasks, you will need to set up automation on your own system.

**Workflow for Daily Updates:**
1.  Run `scraper.py` to get the latest products from Alibaba.
2.  Run `nlp_processor.py` to categorize these new products.
3.  The web application, when the "Load/Refresh Products" button is clicked (or if you implement a CLI command for it), will load this new `categorized_products.json` and handle the 30-day archive.

**To fully automate:**

You need to create a script that runs `scraper.py` and then `nlp_processor.py` sequentially. Then, you can schedule this script to run daily using your operating system's scheduler:

*   **Linux/macOS:** Use `cron`.
    1.  Create a shell script (e.g., `run_daily_tasks.sh`) in the `alibaba_explorer` directory:
        ```sh
        #!/bin/bash
        cd /path/to/your/alibaba_explorer # IMPORTANT: Use absolute path
        source venv/bin/activate # Activate virtual environment
        python3 scraper.py
        python3 nlp_processor.py
        # Optional: Trigger data load into Flask DB via a CLI command if you add one
        # (cd src && export FLASK_APP=main.py && python3 -m flask load-data)
        deactivate
        ```
    2.  Make it executable: `chmod +x run_daily_tasks.sh`
    3.  Edit your crontab: `crontab -e`
    4.  Add a line to run the script daily (e.g., at 2 AM):
        `0 2 * * * /path/to/your/alibaba_explorer/run_daily_tasks.sh >> /path/to/your/alibaba_explorer/cron.log 2>&1`

*   **Windows:** Use Task Scheduler.
    1.  Create a batch file (e.g., `run_daily_tasks.bat`) in the `alibaba_explorer` directory:
        ```bat
        @echo off
        cd C:\path\to\your\alibaba_explorer # IMPORTANT: Use absolute path
        call venv\Scripts\activate.bat
        python scraper.py
        python nlp_processor.py
        REM Optional: Trigger data load into Flask DB via a CLI command
        REM (cd src && set FLASK_APP=main.py && python -m flask load-data)
        call venv\Scripts\deactivate.bat
        ```
    2.  Open Task Scheduler, create a new task, set a daily trigger, and set the action to run this batch file.

**Note on Data Loading for the Web App:** The web application currently loads data from `categorized_products.json` when the "Load/Refresh Products" button is clicked or via the `flask load-data` CLI command. For a fully automated system where the web app always reflects the latest data without manual intervention, you would ideally have the scheduled task also trigger the `flask load-data` command after `nlp_processor.py` completes.

## 7. Future Enhancements

*   **User Accounts:** Implement proper user authentication for personalized favorites.
*   **Advanced NLP:** Improve categorization with more sophisticated models, allow users to define niches, and provide better keyword clustering visualization.
*   **Direct Scraper/NLP Trigger:** Integrate the scraper and NLP scripts more directly into the Flask app (e.g., using Celery or RQ for background tasks) if the hosting environment changes.
*   **Error Handling & Logging:** Enhance logging in all components.
*   **UI/UX Improvements:** More interactive charts, better filtering options, ability to add notes to favorites directly in the UI.
*   **Deployment:** Instructions for deploying to a production server (e.g., using Gunicorn and Nginx).

We hope this tool proves useful for your product research needs!

